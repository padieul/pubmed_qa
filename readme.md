
### A RAG-based system for Pubmed

### Evaluation Metrics

Let's have an example to better understand the below mentioned evaluation metrics.
Text generated by the model = "Students enjoy doing NLP homeworks"
Reference text = "Students enjoy doing homeworks"

- ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
    ROUGE-N compares n-grams of the generated text by the model with n-grams of the reference text. N-grams is basically a chunk of n words. ROUGE measues what percent of the words or n-grams in the reference text occur in the generated output. 

    ROUGE-1 measures the overlap of unigrams (individual words) between the generated text and the reference text. 
    ROUGE-1 (recall) is number of matching words / number of words in reference. In our example ROUGE-1 (recall) is 4/4. Because 4 of the words in the generated text appeared in the reference text, that are "Students", "enjoy", "doing", "homeworks". The number of words in the reference text is 4. 
    ROUGE-1 (precision) is number of matching words / number of words in generated text. In our example, that is 4/5. Again we 4 word mathces, but this time we divide it by the number of words in the generated text, that is 5. 
    ROUGE-1 (F1-score) = 2 * ((precision * recall) / (precision + recall)). That is 0.88888888888. 

    ROUGE-2 measures the overlap of bigrams instead of unigrams. 
    ROUGE-2 (recall) is number of matching bigrams / number of bigrams in reference. In our example ROUGE-1 (recall) is 2/3 because there are 2 matching bigrams that are "Students enjoy" and "enjoy doing". 
    ROUGE-2 (precision) is number of matching bigrams / number of bigrams in generated text. In our example, that is 2/4. We have again 2 bigram mathces and we have 4 bigrams in the generated text.
    Finally, ROUGE-2 (F1-score) = 2 * ((precision * recall) / (precision + recall)). That is 0.57142857142. 
    
    As we can see, the ROUGE-2 scores are lower than ROUGE-1 scores. If the texts are long, ROUGE-2 scores will generally be small because bigrams matches will genereally be a few. It is important to report both ROUGE-1 and ROUGE-2 scores and that is what we do in this project.

    ROUGE-L does not compare n-grams. It treats texts as a sequence of words and then looks for the longest common sequence (LCS). A subsequence is a sequence of words that appears in the same relative order. However, those words are not restricted to be coninuous. In our case "Students enjoy doing homeworks" is the longest common subsequence between the generated text and the reference text.
    Its main advantage over ROUGE-1 and ROUGE-2 is that it does not depend on consecutive n-gram mathces as it can be seen from our example. This provides a more accurate capture of sentences. 
    ROUGE-L (recall) is the number of words in longest common subsequence (LCS) / number of words in reference. The LCS is "Students enjoy doing homeworks". The number of words in LCS is 4. So, ROUGE-L (recall) is 4/4
    ROUGE-L (precision) is the number of words in longest common subsequence (LCS) / number of words in generated text. That is 4/5 in our case. 
    ROUGE-L (F1-score) = 2 * ((precision * recall) / (precision + recall)). That is 0.88888888888.


- BLEU (Bilingual Evaluation Understudy)
    BLEU compares n-grams of the generated text to the n-grams of the reference text. 
    

- BERTScore