
### A RAG-based system for Pubmed

### Evaluation Metrics

Let's have an example to better understand the below mentioned evaluation metrics.
Text generated by the model = "Students enjoy doing NLP homeworks"
Reference text = "Students enjoy doing homeworks"

- ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
    ROUGE-N compares n-grams of the generated text by the model with n-grams of the reference text. N-grams is basically a chunk of n words. ROUGE measues what percent of the words or n-grams in the reference text occur in the generated output. 

    ROUGE-1 measures the overlap of unigrams (individual words) between the generated text and the reference text. 
    ROUGE-1 (recall) is number of matching words / number of words in reference. In our example ROUGE-1 (recall) is 4/4. Because 4 of the words in the generated text appeared in the reference text, that are "Students", "enjoy", "doing", "homeworks". The number of words in the reference text is 4. 
    ROUGE-1 (precision) is number of matching words / number of words in generated text. In our example, that is 4/5. Again we 4 word mathces, but this time we divide it by the number of words in the generated text, that is 5. 
    ROUGE-1 (F1-score) = 2 * ((precision * recall) / (precision + recall)). That is 0.88888888888. 

    ROUGE-2 measures the overlap of bigrams instead of unigrams. 
    ROUGE-2 (recall) is number of matching bigrams / number of bigrams in reference. In our example ROUGE-1 (recall) is 2/3 because there are 2 matching bigrams that are "Students enjoy" and "enjoy doing". 
    ROUGE-2 (precision) is number of matching bigrams / number of bigrams in generated text. In our example, that is 2/4. We have again 2 bigram mathces and we have 4 bigrams in the generated text.
    Finally, ROUGE-2 (F1-score) = 2 * ((precision * recall) / (precision + recall)). That is 0.57142857142. 
    
    As we can see, the ROUGE-2 scores are lower than ROUGE-1 scores. If the texts are long, ROUGE-2 scores will generally be small because bigrams matches will genereally be a few. It is important to report both ROUGE-1 and ROUGE-2 scores and that is what we do in this project.

    ROUGE-L does not compare n-grams. It treats texts as a sequence of words and then looks for the longest common sequence (LCS). A subsequence is a sequence of words that appears in the same relative order. However, those words are not restricted to be coninuous. In our case "Students enjoy doing homeworks" is the longest common subsequence between the generated text and the reference text.
    Its main advantage over ROUGE-1 and ROUGE-2 is that it does not depend on consecutive n-gram mathces as it can be seen from our example. This provides a more accurate capture of sentences. 
    ROUGE-L (recall) is the number of words in longest common subsequence (LCS) / number of words in reference. The LCS is "Students enjoy doing homeworks". The number of words in LCS is 4. So, ROUGE-L (recall) is 4/4
    ROUGE-L (precision) is the number of words in longest common subsequence (LCS) / number of words in generated text. That is 4/5 in our case. 
    ROUGE-L (F1-score) = 2 * ((precision * recall) / (precision + recall)). That is 0.88888888888.


- BLEU (Bilingual Evaluation Understudy)
    BLEU is used to have a single numerical value to a generated text tells how good it is compared to one or more reference texts. BLEU compares n-grams of the generated text to the n-grams of the reference text.
    
    Let's start with uniqrams which correponds to individual words in a text. Again we see that 4 of the words in the generated text appear in the reference text. That are "Students", "enjoy", "doing", "homeworks". 
    The precision or the Unigram Precision is number of matching words / number of words in the generated text. In this case, that is 4/5. The higher precision means a better generation of text. 
    However, a problem that may arise is that repetitive patterns may occur, meaning a word can occur several times resulting in high precision. However, this does not mean that the generated text is good. As an example, if the model generates "homework homework homework homework" the precision gets a perfect score of 1, even the generated text is terribly bad. 
    To handle this issue, BLEU uses a modified precision that clips the number of times to count a specific word based on the number of times it appears in the reference text. In our example, "homework" appears only once in the reference text. So, BLEU clips the number of word mathces to 1 and give the modified unigram precision that is 1/4 (that is much lower than the previous precision as it should be). 
    Another problem is that unigram precision does not pay attention to the order of words. Imagine we have a generated text that is "doing NLP Students homeworks enjoy". In this case, we again get a high score of 4/5 which is not something expected.
    To handle this issue, BLEU actually computes the precision (modified precision) for several n-grams not just unigrams and then reports the results.
    If we calculate the trigram (3-gram) precision for our generated text of "doing NLP Students homeworks enjoy", we get a precision of 0/5 which is something expected. That is because no 3 chunks of words appear in the reference text that we have. So, here the order is considered.

    BLEU calculates unigram, bigram, trigram, 4-gram precision scores and reports individual precision scores and a BLEU Score that is the geometric mean of all four n-gram precisions. BLEU Score is easy to compute and popular but it does not consider meaning and incorporate sentence structure. 

- BERTScore